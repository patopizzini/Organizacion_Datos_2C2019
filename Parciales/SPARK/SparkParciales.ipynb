{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "try: \n",
    "    type(sc)\n",
    "except NameError:\n",
    "    sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primer Cuatrimestre de 2019. Examen parcial, primera oportunidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(_c0=u'1', _c1=u'NOMBRE_1', _c2=u'CABA', _c3=u'CABA', _c4=u'0,5', _c5=u'0,5'), Row(_c0=u'2', _c1=u'NOMBRE_2', _c2=u'CABA', _c3=u'CABA', _c4=u'0,5', _c5=u'0,5'), Row(_c0=u'3', _c1=u'NOMBRE_3', _c2=u'CABA', _c3=u'CABA', _c4=u'0,5', _c5=u'0,5'), Row(_c0=u'4', _c1=u'NOMBRE_4', _c2=u'BUENOSAIRES', _c3=u'LAPLATA', _c4=u'0,5', _c5=u'0,5'), Row(_c0=u'5', _c1=u'NOMBRE_5', _c2=u'SANTAFE', _c3=u'ROSARIO', _c4=u'0,5', _c5=u'0,5')]\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.csv(\"bases.csv\",header=False,sep=\";\");\n",
    "print(df1.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(_c0=u'10/10/2019', _c1=u'1', _c2=u'25', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'), Row(_c0=u'11/10/2019', _c1=u'1', _c2=u'20', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'), Row(_c0=u'10/10/2019', _c1=u'1', _c2=u'25', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'), Row(_c0=u'10/10/2018', _c1=u'4', _c2=u'25', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'), Row(_c0=u'11/10/2018', _c1=u'4', _c2=u'20', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'), Row(_c0=u'12/10/2018', _c1=u'4', _c2=u'30', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'), Row(_c0=u'10/11/2018', _c1=u'4', _c2=u'25', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'), Row(_c0=u'11/11/2018', _c1=u'4', _c2=u'20', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'), Row(_c0=u'12/11/2018', _c1=u'4', _c2=u'15', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'), Row(_c0=u'10/12/2018', _c1=u'4', _c2=u'10', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'), Row(_c0=u'11/12/2018', _c1=u'4', _c2=u'10', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'), Row(_c0=u'12/12/2018', _c1=u'4', _c2=u'10', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'), Row(_c0=u'13/12/2018', _c1=u'4', _c2=u'10', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'), Row(_c0=u'10/10/2019', _c1=u'1', _c2=u'25', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'), Row(_c0=u'10/10/2019', _c1=u'1', _c2=u'25', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'), Row(_c0=u'10/10/2019', _c1=u'1', _c2=u'25', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35')]\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.csv(\"mediciones.csv\",header=False,sep=\";\");\n",
    "print(df2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posible solución.\n",
    "\n",
    "a. Map por ID_BASE en ambos RDD.\n",
    "\n",
    "b.Filter en RDD bases por PCIA = Buenos Aires.\n",
    "\n",
    "c.Filter en RDD mediciones por TIMESTAMP entre 01-12-2017 y 31-12-2018.\n",
    "\n",
    "d.Inner join entre ambos RDD.\n",
    "\n",
    "e.MAP por ID_BASE, YEAR(TIMESTAMP), MONTH(TIMESTAMP).\n",
    "\n",
    "f.ReduceByKey para calcular suma y contador.\n",
    "\n",
    "g.MAP para cambiar la clave a ID_BASE y aprovechar para hacer el calculo del promedio. En los values debe\n",
    "quedar el promedio, el nombre de la base y el mes/año.\n",
    "\n",
    "h.GroupByKey\n",
    "\n",
    "i.Filter para comparar que la tasa de un mes a otro haya variado en 50%.\n",
    "\n",
    "j.Map para quedarse con ID_BASE y nombre.\n",
    "\n",
    "Map innecesarios, descuento de 3 puntos. Filter tardíos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bases = df1.rdd\n",
    "mediciones = df2.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=u'1', _c1=u'NOMBRE_1', _c2=u'CABA', _c3=u'CABA', _c4=u'0,5', _c5=u'0,5'),\n",
       " Row(_c0=u'2', _c1=u'NOMBRE_2', _c2=u'CABA', _c3=u'CABA', _c4=u'0,5', _c5=u'0,5'),\n",
       " Row(_c0=u'3', _c1=u'NOMBRE_3', _c2=u'CABA', _c3=u'CABA', _c4=u'0,5', _c5=u'0,5'),\n",
       " Row(_c0=u'4', _c1=u'NOMBRE_4', _c2=u'BUENOSAIRES', _c3=u'LAPLATA', _c4=u'0,5', _c5=u'0,5'),\n",
       " Row(_c0=u'5', _c1=u'NOMBRE_5', _c2=u'SANTAFE', _c3=u'ROSARIO', _c4=u'0,5', _c5=u'0,5')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bases.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=u'10/10/2019', _c1=u'1', _c2=u'25', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'),\n",
       " Row(_c0=u'11/10/2019', _c1=u'1', _c2=u'20', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'),\n",
       " Row(_c0=u'10/10/2019', _c1=u'1', _c2=u'25', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'),\n",
       " Row(_c0=u'10/10/2018', _c1=u'4', _c2=u'25', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'),\n",
       " Row(_c0=u'11/10/2018', _c1=u'4', _c2=u'20', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'),\n",
       " Row(_c0=u'12/10/2018', _c1=u'4', _c2=u'30', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'),\n",
       " Row(_c0=u'10/11/2018', _c1=u'4', _c2=u'25', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'),\n",
       " Row(_c0=u'11/11/2018', _c1=u'4', _c2=u'20', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'),\n",
       " Row(_c0=u'12/11/2018', _c1=u'4', _c2=u'15', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'),\n",
       " Row(_c0=u'10/12/2018', _c1=u'4', _c2=u'10', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'),\n",
       " Row(_c0=u'11/12/2018', _c1=u'4', _c2=u'10', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'),\n",
       " Row(_c0=u'12/12/2018', _c1=u'4', _c2=u'10', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'),\n",
       " Row(_c0=u'13/12/2018', _c1=u'4', _c2=u'10', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'),\n",
       " Row(_c0=u'10/10/2019', _c1=u'1', _c2=u'25', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'),\n",
       " Row(_c0=u'10/10/2019', _c1=u'1', _c2=u'25', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'),\n",
       " Row(_c0=u'10/10/2019', _c1=u'1', _c2=u'25', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mediciones.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bases = bases.filter(lambda x: x[2] == 'BUENOSAIRES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=u'4', _c1=u'NOMBRE_4', _c2=u'BUENOSAIRES', _c3=u'LAPLATA', _c4=u'0,5', _c5=u'0,5')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bases.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mediciones = mediciones.filter(lambda x: x[0].endswith('2018'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=u'10/10/2018', _c1=u'4', _c2=u'25', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'),\n",
       " Row(_c0=u'11/10/2018', _c1=u'4', _c2=u'20', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'),\n",
       " Row(_c0=u'12/10/2018', _c1=u'4', _c2=u'30', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'),\n",
       " Row(_c0=u'10/11/2018', _c1=u'4', _c2=u'25', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'),\n",
       " Row(_c0=u'11/11/2018', _c1=u'4', _c2=u'20', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'),\n",
       " Row(_c0=u'12/11/2018', _c1=u'4', _c2=u'15', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'),\n",
       " Row(_c0=u'10/12/2018', _c1=u'4', _c2=u'10', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'),\n",
       " Row(_c0=u'11/12/2018', _c1=u'4', _c2=u'10', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'),\n",
       " Row(_c0=u'12/12/2018', _c1=u'4', _c2=u'10', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35'),\n",
       " Row(_c0=u'13/12/2018', _c1=u'4', _c2=u'10', _c3=u'30', _c4=u'35', _c5=u'35', _c6=u'35')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mediciones.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bases = bases.map(lambda x: (x[0],(x[1],x[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'4', (u'NOMBRE_4', u'LAPLATA'))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bases.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mediciones = mediciones.map(lambda x: (x[1],(x[2],x[0][3:5],x[0][6:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'4', (u'25', u'10', u'2018')),\n",
       " (u'4', (u'20', u'10', u'2018')),\n",
       " (u'4', (u'30', u'10', u'2018')),\n",
       " (u'4', (u'25', u'11', u'2018')),\n",
       " (u'4', (u'20', u'11', u'2018')),\n",
       " (u'4', (u'15', u'11', u'2018')),\n",
       " (u'4', (u'10', u'12', u'2018')),\n",
       " (u'4', (u'10', u'12', u'2018')),\n",
       " (u'4', (u'10', u'12', u'2018')),\n",
       " (u'4', (u'10', u'12', u'2018'))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mediciones.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "joins = bases.leftOuterJoin(mediciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'4', ((u'NOMBRE_4', u'LAPLATA'), (u'25', u'10', u'2018'))),\n",
       " (u'4', ((u'NOMBRE_4', u'LAPLATA'), (u'20', u'10', u'2018'))),\n",
       " (u'4', ((u'NOMBRE_4', u'LAPLATA'), (u'30', u'10', u'2018'))),\n",
       " (u'4', ((u'NOMBRE_4', u'LAPLATA'), (u'25', u'11', u'2018'))),\n",
       " (u'4', ((u'NOMBRE_4', u'LAPLATA'), (u'20', u'11', u'2018'))),\n",
       " (u'4', ((u'NOMBRE_4', u'LAPLATA'), (u'15', u'11', u'2018'))),\n",
       " (u'4', ((u'NOMBRE_4', u'LAPLATA'), (u'10', u'12', u'2018'))),\n",
       " (u'4', ((u'NOMBRE_4', u'LAPLATA'), (u'10', u'12', u'2018'))),\n",
       " (u'4', ((u'NOMBRE_4', u'LAPLATA'), (u'10', u'12', u'2018'))),\n",
       " (u'4', ((u'NOMBRE_4', u'LAPLATA'), (u'10', u'12', u'2018')))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joins.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "joins = joins.map(lambda x: ((x[0],x[1][1][1],x[1][1][2]),(x[1][1][0],1,x[1][0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'4', u'10', u'2018'), (u'25', 1, u'NOMBRE_4')),\n",
       " ((u'4', u'10', u'2018'), (u'20', 1, u'NOMBRE_4')),\n",
       " ((u'4', u'10', u'2018'), (u'30', 1, u'NOMBRE_4')),\n",
       " ((u'4', u'11', u'2018'), (u'25', 1, u'NOMBRE_4')),\n",
       " ((u'4', u'11', u'2018'), (u'20', 1, u'NOMBRE_4')),\n",
       " ((u'4', u'11', u'2018'), (u'15', 1, u'NOMBRE_4')),\n",
       " ((u'4', u'12', u'2018'), (u'10', 1, u'NOMBRE_4')),\n",
       " ((u'4', u'12', u'2018'), (u'10', 1, u'NOMBRE_4')),\n",
       " ((u'4', u'12', u'2018'), (u'10', 1, u'NOMBRE_4')),\n",
       " ((u'4', u'12', u'2018'), (u'10', 1, u'NOMBRE_4'))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joins.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "joins = joins.reduceByKey(lambda x, y: (int(x[0])+int(y[0]),x[1]+y[1],x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'4', u'10', u'2018'), (75, 3, u'NOMBRE_4')),\n",
       " ((u'4', u'12', u'2018'), (40, 4, u'NOMBRE_4')),\n",
       " ((u'4', u'11', u'2018'), (60, 3, u'NOMBRE_4'))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joins.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "joins = joins.map(lambda x: (x[0][0],(x[1][0]/x[1][1], x[1][2], x[0][1],x[0][2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'4', (25, u'NOMBRE_4', u'10', u'2018')),\n",
       " (u'4', (10, u'NOMBRE_4', u'12', u'2018')),\n",
       " (u'4', (20, u'NOMBRE_4', u'11', u'2018'))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joins.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = joins.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'4', <pyspark.resultiterable.ResultIterable at 0x7ff498935fd0>)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_10p(x):\n",
    "    data = list(x[1])\n",
    "    print(data)\n",
    "    if int(data[1][2]) == 12:\n",
    "        return (abs(int(data[1][0]) - int(data[2][0])) >= 0.5*int(data[2][0]))\n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = group.filter(filter_10p).map(lambda x: (x[0], list(x[1])[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'4', u'NOMBRE_4')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segundo Cuatrimestre de 2019. Examen parcial, primera oportunidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(_c0=u'1', _c1=u'nombre_1', _c2=u'10/10/1990', _c3=u'ARGENTINA'), Row(_c0=u'2', _c1=u'nombre_2', _c2=u'10/10/1990', _c3=u'ARGENTINA'), Row(_c0=u'3', _c1=u'nombre_3', _c2=u'10/10/1990', _c3=u'BOLIVIA'), Row(_c0=u'4', _c1=u'nombre_4', _c2=u'10/10/1990', _c3=u'BOLIVIA'), Row(_c0=u'5', _c1=u'nombre_5', _c2=u'10/10/1990', _c3=u'CHILE'), Row(_c0=u'6', _c1=u'nombre_6', _c2=u'10/10/1990', _c3=u'CHILE')]\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.read.csv(\"jugadores.csv\",header=False,sep=\";\");\n",
    "print(df3.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(_c0=u'1', _c1=u'1', _c2=u'20', _c3=u'200', _c4=u'5'), Row(_c0=u'2', _c1=u'1', _c2=u'20', _c3=u'200', _c4=u'5'), Row(_c0=u'3', _c1=u'1', _c2=u'20', _c3=u'200', _c4=u'5'), Row(_c0=u'1', _c1=u'2', _c2=u'20', _c3=u'200', _c4=u'5'), Row(_c0=u'2', _c1=u'2', _c2=u'10', _c3=u'200', _c4=u'5'), Row(_c0=u'3', _c1=u'2', _c2=u'10', _c3=u'200', _c4=u'5')]\n"
     ]
    }
   ],
   "source": [
    "df4 = spark.read.csv(\"estadisticas.csv\",header=False,sep=\";\");\n",
    "print(df4.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(_c0=u'1', _c1=u'2018', _c2=u'1', _c3=u'5', _c4=u'lanus', _c5=u'banfield'), Row(_c0=u'2', _c1=u'2018', _c2=u'2', _c3=u'10', _c4=u'temperley', _c5=u'losandes'), Row(_c0=u'3', _c1=u'2018', _c2=u'2', _c3=u'15', _c4=u'quilmes', _c5=u'argentinos'), Row(_c0=u'4', _c1=u'2019', _c2=u'1', _c3=u'3', _c4=u'allboys', _c5=u'chicago')]\n"
     ]
    }
   ],
   "source": [
    "df5 = spark.read.csv(\"partidos.csv\",header=False,sep=\";\");\n",
    "print(df5.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "jugadores = df3.rdd\n",
    "estadisticas = df4.rdd\n",
    "partidos = df5.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=u'1', _c1=u'nombre_1', _c2=u'10/10/1990', _c3=u'ARGENTINA'),\n",
       " Row(_c0=u'2', _c1=u'nombre_2', _c2=u'10/10/1990', _c3=u'ARGENTINA'),\n",
       " Row(_c0=u'3', _c1=u'nombre_3', _c2=u'10/10/1990', _c3=u'BOLIVIA'),\n",
       " Row(_c0=u'4', _c1=u'nombre_4', _c2=u'10/10/1990', _c3=u'BOLIVIA'),\n",
       " Row(_c0=u'5', _c1=u'nombre_5', _c2=u'10/10/1990', _c3=u'CHILE'),\n",
       " Row(_c0=u'6', _c1=u'nombre_6', _c2=u'10/10/1990', _c3=u'CHILE')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jugadores.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "jugadores = jugadores.filter(lambda x: x[3] == 'ARGENTINA')\n",
    "jugadores = jugadores.map(lambda x: (x[0],(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'1', u'nombre_1'), (u'2', u'nombre_2')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jugadores.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=u'1', _c1=u'1', _c2=u'20', _c3=u'200', _c4=u'5'),\n",
       " Row(_c0=u'2', _c1=u'1', _c2=u'20', _c3=u'200', _c4=u'5'),\n",
       " Row(_c0=u'3', _c1=u'1', _c2=u'20', _c3=u'200', _c4=u'5'),\n",
       " Row(_c0=u'1', _c1=u'2', _c2=u'20', _c3=u'200', _c4=u'5'),\n",
       " Row(_c0=u'2', _c1=u'2', _c2=u'10', _c3=u'200', _c4=u'5'),\n",
       " Row(_c0=u'3', _c1=u'2', _c2=u'10', _c3=u'200', _c4=u'5')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estadisticas.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "estadisticas = estadisticas.map(lambda x: (x[1],(x[0],x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'1', (u'1', u'20')),\n",
       " (u'1', (u'2', u'20')),\n",
       " (u'1', (u'3', u'20')),\n",
       " (u'2', (u'1', u'20')),\n",
       " (u'2', (u'2', u'10')),\n",
       " (u'2', (u'3', u'10'))]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estadisticas.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=u'1', _c1=u'2018', _c2=u'1', _c3=u'5', _c4=u'lanus', _c5=u'banfield'),\n",
       " Row(_c0=u'2', _c1=u'2018', _c2=u'2', _c3=u'10', _c4=u'temperley', _c5=u'losandes'),\n",
       " Row(_c0=u'3', _c1=u'2018', _c2=u'2', _c3=u'15', _c4=u'quilmes', _c5=u'argentinos'),\n",
       " Row(_c0=u'4', _c1=u'2019', _c2=u'1', _c3=u'3', _c4=u'allboys', _c5=u'chicago')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partidos.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "partidos = partidos.map(lambda x: (x[0],(x[2],x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'1', (u'1', u'2018')),\n",
       " (u'2', (u'2', u'2018')),\n",
       " (u'3', (u'2', u'2018')),\n",
       " (u'4', (u'1', u'2019'))]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partidos.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg = jugadores.leftOuterJoin(estadisticas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'1', (u'nombre_1', (u'1', u'20'))),\n",
       " (u'1', (u'nombre_1', (u'2', u'20'))),\n",
       " (u'1', (u'nombre_1', (u'3', u'20'))),\n",
       " (u'2', (u'nombre_2', (u'1', u'20'))),\n",
       " (u'2', (u'nombre_2', (u'2', u'10'))),\n",
       " (u'2', (u'nombre_2', (u'3', u'10')))]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg = arg.map(lambda x: ((x[1][1][0]),(x[0],x[1][0],x[1][1][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'1', (u'1', u'nombre_1', u'20')),\n",
       " (u'2', (u'1', u'nombre_1', u'20')),\n",
       " (u'3', (u'1', u'nombre_1', u'20')),\n",
       " (u'1', (u'2', u'nombre_2', u'20')),\n",
       " (u'2', (u'2', u'nombre_2', u'10')),\n",
       " (u'3', (u'2', u'nombre_2', u'10'))]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = arg.leftOuterJoin(partidos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'2', ((u'1', u'nombre_1', u'20'), (u'2', u'2018'))),\n",
       " (u'2', ((u'2', u'nombre_2', u'10'), (u'2', u'2018'))),\n",
       " (u'1', ((u'1', u'nombre_1', u'20'), (u'1', u'2018'))),\n",
       " (u'1', ((u'2', u'nombre_2', u'20'), (u'1', u'2018'))),\n",
       " (u'3', ((u'1', u'nombre_1', u'20'), (u'2', u'2018'))),\n",
       " (u'3', ((u'2', u'nombre_2', u'10'), (u'2', u'2018')))]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = full.map(lambda x: ((x[1][0][0],x[1][1][0],x[1][1][1]),(x[1][0][1],x[1][0][2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'1', u'2', u'2018'), (u'nombre_1', u'20')),\n",
       " ((u'2', u'2', u'2018'), (u'nombre_2', u'10')),\n",
       " ((u'1', u'1', u'2018'), (u'nombre_1', u'20')),\n",
       " ((u'2', u'1', u'2018'), (u'nombre_2', u'20')),\n",
       " ((u'1', u'2', u'2018'), (u'nombre_1', u'20')),\n",
       " ((u'2', u'2', u'2018'), (u'nombre_2', u'10'))]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = full.reduceByKey(lambda x,y: ( x[0], int(x[1])+int(y[1]) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'2', u'2', u'2018'), (u'nombre_2', 20)),\n",
       " ((u'1', u'1', u'2018'), (u'nombre_1', u'20')),\n",
       " ((u'1', u'2', u'2018'), (u'nombre_1', 40)),\n",
       " ((u'2', u'1', u'2018'), (u'nombre_2', u'20'))]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = full.map(lambda x: ( (x[0][0],x[1][0]),(x[0][1],x[0][2],int(x[1][1])) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'2', u'nombre_2'), (u'2', u'2018', 20)),\n",
       " ((u'1', u'nombre_1'), (u'1', u'2018', 20)),\n",
       " ((u'1', u'nombre_1'), (u'2', u'2018', 40)),\n",
       " ((u'2', u'nombre_2'), (u'1', u'2018', 20))]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = full.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'2', u'nombre_2'),\n",
       "  <pyspark.resultiterable.ResultIterable at 0x7ff49881ead0>),\n",
       " ((u'1', u'nombre_1'),\n",
       "  <pyspark.resultiterable.ResultIterable at 0x7ff49881ee90>)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_Futbol(x):\n",
    "    data = list(x[1])\n",
    "    if int(data[0][0]) == 2:\n",
    "        return (abs(int(data[0][2]) - int(data[1][2])) >= 0.5*int(data[1][2]))\n",
    "    else: \n",
    "        return (abs(int(data[1][2]) - int(data[0][2])) >= 0.5*int(data[0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = full.filter(filter_Futbol).map(lambda x:x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 147.0 failed 1 times, most recent failure: Lost task 2.0 in stage 147.0 (TID 159, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/andres/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/home/andres/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/andres/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-110-e5e6cd7e6488>\", line 6, in filter_Futbol\nTypeError: 'bool' object is not callable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor87.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/andres/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/home/andres/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/andres/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-110-e5e6cd7e6488>\", line 6, in filter_Futbol\nTypeError: 'bool' object is not callable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-781f71c5a189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresultado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/andres/spark-2.3.0-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \"\"\"\n\u001b[1;32m    823\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/andres/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/andres/spark-2.3.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/andres/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 147.0 failed 1 times, most recent failure: Lost task 2.0 in stage 147.0 (TID 159, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/andres/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/home/andres/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/andres/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-110-e5e6cd7e6488>\", line 6, in filter_Futbol\nTypeError: 'bool' object is not callable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor87.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/andres/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/home/andres/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/andres/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-110-e5e6cd7e6488>\", line 6, in filter_Futbol\nTypeError: 'bool' object is not callable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "resultado.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
